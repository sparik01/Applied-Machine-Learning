{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied Machine Learning: Assignment \\#2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sheetal Parikh\n",
    "EN.605.631.81<br>\n",
    "February 8, 2021\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "*At a high-level, without entering into mathematical details, compare and contrast the following classifiers:*\n",
    "- *Perceptron (textbook's version)*\n",
    "- *SVM* \n",
    "- *Decision Tree* \n",
    "- *Random Forest (you have to research a bit about this classifier)* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perceptron\n",
    "- Overview - Perceptron is a supervised classification algorithm. Perceptron's are the foundation of other algorithms such as neural networks and SVM and can be used to distinguish between two linearly separable classes by predicting whether a data point belongs to one class or another. Perceptron stops after it classifies data correctly. Neurons are similar to perceptrons as neurons either fire or they don't.  \n",
    "\n",
    "- Advantages - The model can take in real-valued inputs as well as boolean values.  The generalization of the model helps prevent overfitting. If the training set is linearly separable than the perceptron will always converge. If there is no good linear separator, in certain situations the weights of the inputs can be adjusted to achieve linear separation.\n",
    "\n",
    "- Disadvantages - The output can only be boolean values since input belongs to one class or another. Furthermore, if the training set is not linearly separable, then the perceptron will never converge.\n",
    "\n",
    "#### SVM\n",
    "- Overview - SVM is a supervised classification algorithm that separates clusters of observations with a hyperplane or decision boundary.  It finds a hyperplane that maximizes the distance between the points of two classes and stops after finding the best plane that has the maximum distance. The perceptron algorithm is a building block of SVM.\n",
    "\n",
    "- Advantages - SVM is more robust than perceptron in getting to the real target function and can can be used to solve both linear and non-linear problems. Having decision boundaries with large margin help prevent overfitting. Furthermore, SVM can be used in both classification and regression problems. SVM works well when the training data is less and you have a large number of features and tends to have high accuracy. SVM also can handle outliers.\n",
    "\n",
    "- Disadvantages - SVM can be memory intensive and does not scale well for large datasets and does not perform well when the data set has a lot of noise.  Learning can be slow for SVM. Also the structure of the algorithm can be difficult to understand.\n",
    "\n",
    "#### Decision Tree\n",
    "- Overview - Decision Tree is a supervised classification algorithm which breaks down data into smaller susets by making decisions by asking a series of questions.  This develops a tree structure with decision nodes and leaf nodes.\n",
    "\n",
    "- Advantages - Decision trees can work well even if the data is not preprocessed and therefore less effort is required in preprocessing the data. The algorithm can handle colinearity and outliers well and also works well for categorical values.  The algorithm achieves better accuracy when you have less noise. If the dataset is not complex, the algorithm can be simple and intuitive. Decision trees can be used for both regression and classification problems.\n",
    "\n",
    "- Disadvantages - Decision trees are prone to overfitting.  Complex data sets can result in large trees with a lot of splits. These trees can be complex and may result in overfitting. Decision trees can also lose information when handling continuous variables and does not have high accuracy. Learning can be slow for decision trees.\n",
    "\n",
    "\n",
    "#### Random Forest\n",
    "- Overview - Random Forest is a supervised classification ensembling algorithm that is an ensemble of decision trees.  Ensembling algorithms combine the results of multiple learners for improved results. Each decision tree predicts a value and then the average of the predictions are chosen.\n",
    "\n",
    "- Advantages - Decision trees are less prone to overfitting than decision trees and also provides a more generalized solution. The algorithm tends to be more robust than decision trees and has higher accuracy. Random forest can work with both categorical and continuous variables and can be used for both classification and regression problems.  The algorithm can also handle both linear and non-linear parameters.\n",
    "\n",
    "- Disadvantages - Learning can be slow for the random forest algorithm as many trees are generated.  Also, this requires a lot of computational power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "\n",
    "Raschka, Sebastian; Mirjalili, Vahid. Python Machine Learning: Machine Learning and Deep Learning with Python, scikit-learn, and TensorFlow 2, 3rd Edition (p. 168). Packt Publishing. Kindle Edition.\n",
    "\n",
    "https://medium.com/fintechexplained/machine-learning-algorithm-comparison-f14ce372b855\n",
    "\n",
    "https://elitedatascience.com/machine-learning-algorithms\n",
    "\n",
    "https://towardsdatascience.com/comparative-study-on-classic-machine-learning-algorithms-24f9ff6ab222\n",
    "\n",
    "https://medium.com/@subashkharel/perceptron-vs-svm-a-quick-comparison-6b5d6b5d64f\n",
    "\n",
    "https://towardsdatascience.com/a-complete-view-of-decision-trees-and-svm-in-machine-learning-f9f3d19a337b\n",
    "\n",
    "https://builtin.com/data-science/supervised-machine-learning-classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "*Using real datasets (can also be hypothetically constructed by yourself) define the following feature types, and give example values from your dataset. How would you represent these features in a computer program? (e.g. 32-bit integer? Floating point? String?)*\n",
    "\n",
    "- *Numerical*\n",
    "- *Nominal*\n",
    "- *Date*\n",
    "- *Text*\n",
    "- *Image*\n",
    "- *Dependent variable*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numerical\n",
    "- Numerical features are numbers(int, double, floating point) that represent a measurement or count. Numerical features can be either continuous or discrete and are not ordered in time.  Continuous features are values that can be divided into smaller increments and so can be decimal or fractional values.  For example, measurements of height or weight are continuous features. Discrete data are a count of the presence of a activities, characteristics, items, etc.and so cannot be divided into fractional values. For example, the Iris dataset we used in Assignment 1, included floating point numbers to represent sepal length, sepal width, petal length and petal width. Furthermore, the following dataset from Kaggle represents information onAmazon's Top 50 bestselling books from 2009 to 2019(https://www.kaggle.com/sootersaalu/amazon-top-50-bestselling-books-2009-2019).  The number of written reviews on Amazon are ints that represents a discrete value because the number of reviews cannot be a fraction.<br>\n",
    "\n",
    "#### Nominal\n",
    "- Nominal features are discrete categorical units that have no quantitiative value and have no order. Nominal features are represented as strings or can be mapped to an int. For example animal species or nationality are nominal features. Stats of mean surface temperature by country from 1961 -2019. For example, the amazon review dataset from above includes a column for genre which represents a nominal feature that is a string.<br>\n",
    "\n",
    "#### Date\n",
    "- Features that are dates are time series data that was collected at set intervals over a period of time.  Dates do not have implied ordering and can be represented by numerical or string values.  For example, the following dataset from kaggle lists tv shows and movies available on Netflix as of 2019.  The column for when the show or movie was added onto netflix represents a date that is in string format (\"April 1, 2020\").<br>\n",
    "https://www.kaggle.com/shivamb/netflix-shows\n",
    "\n",
    "#### Text\n",
    "- Text data can be words, sentences,paragraphs or documents and is unstructured because you have a variety of words and lengths. Text data is represented by strings but can be converted to numeric(ints) vectors. For example, if you have text documents, each document can be represented by a vector row. For example, the following dataset from Kaggle is comprised of coronavirus tagged tweets from twitter.  The column for the original text of the tweet represents text data that is a string. <br>\n",
    "https://www.kaggle.com/datatattle/covid-19-nlp-text-classification\n",
    "\n",
    "#### Image\n",
    "- Images are multidimensional arrays of pixels. These arrays include numerical values. A grayscale image contains numbers that give the pixel intensity values.  Colored images contain three channels (Red, Green, and Blue) and each channel of each pixel contains a value between 0 to 255.  For example, the following dataset from the UCI archive includes black and white face images of people with varying poses, expressions, eyes (whether wearing sunglasses), and size. \n",
    "http://archive.ics.uci.edu/ml/datasets/cmu+face+images <br>\n",
    "\n",
    "#### Dependent Variable\n",
    "- Dependent variables are the target variables we are studying and trying to predict.  For example, if we had a dataset of the final grade in History class based on the number of hours studied, number of classes attended, and number of homework assignments completed, the dependent variable would be the final grade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "\n",
    "https://medium.com/analytics-vidhya/built-in-datasets-in-python-24adca42012c\n",
    "\n",
    "https://scikit-learn.org/stable/datasets/real_world.html\n",
    "\n",
    "https://towardsdatascience.com/data-types-from-a-machine-learning-perspective-with-examples-111ac679e8bc\n",
    "\n",
    "https://medium.com/swlh/data-types-in-statistics-used-for-machine-learning-5b4c24ae6036\n",
    "\n",
    "https://towardsdatascience.com/understanding-feature-engineering-part-3-traditional-methods-for-text-data-f6f7d70acd41"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "\n",
    "*Using online resources, research and find other classifier performance metrics which are also as common as the accuracy metric. Write down the mathematical equations and the meaning of the metrics that you found.*\n",
    "\n",
    "Accuracy is one of the most common classifier performance metrics and measure the percentage of correctly classified test instances.  However, accuracy does not take into account class distribution and so can be biased to the class with more instances.  There are many other classifier performance metrics that can be used to evaluate performance depending on the type of date.  These metrics can be organized into three types: Threshold, Probability, and Ranking in which threshold and ranking are the most common. Accuracy is categorized as a threshold metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold Metrics Other Than Accuracy\n",
    "Threshold metrics quantify classification prediction errors.\n",
    "\n",
    "The following variables will be used for all the formulas below:\n",
    "    <br><br>*fp = false positive*\n",
    "    <br>*tp = true positive*\n",
    "    <br>*fn = false negative*\n",
    "    <br>*tn = true negative*\n",
    "\n",
    "- Error Rate(err) - The error rate is the complement of accuracy and measures the ratio of incorrect predictions over the number of instances: \n",
    "\n",
    "$$Error Rate =\\frac{tp + fn}{tp + fp + tn + fn}$$\n",
    "\n",
    "- Precision(p) - Precision measures a measure of the relevant data points by finding the ratio of true positive instances over all positive instances (both true and false positives):\n",
    "\n",
    "$$Precision =\\frac{tp}{tp + fp}$$\n",
    "\n",
    "- Recall(r)/Sensitivity(sn) - Recall or sensitivity measures how accurately the model identifies true positive or relative instances:\n",
    "\n",
    "$$Recall =\\frac{tp}{tp + fn}$$\n",
    "\n",
    "- F-Score/F-Measure - The F-Score combines both recall and precision into one metric and is widely used in information retrieval. It finds the harmonic mean between precision and recall. \n",
    "\n",
    "$$F-Score =\\frac{2 * p * r}{p + r}$$\n",
    "\n",
    "- Specificity(sp) - Specificity measures the fraction correctlly classified negative instances:\n",
    "\n",
    "$$Specificity =\\frac{tn}{tn + fp}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability Metrics\n",
    "\n",
    "Probability metrics quantify the uncertainty in a classifierâ€™s predictions.\n",
    "\n",
    "- Mean Square Error (MSE) - The mean square error finds the average squared error between the true value and value predicted by the classifier:\n",
    "\n",
    "$$MSE =\\frac{1}{n} \\sum_{j=1}^{n} (P_j - A_j)^2$$\n",
    "\n",
    "$$n = total\\ number\\ of\\ instances$$\n",
    "$$P_j = predicted\\ value\\ of\\ instance\\ j$$\n",
    "$$A_j = real\\ value\\ of\\ instance\\ j$$\n",
    "\n",
    "- Mean Absolute Error (MAE) - The mean absolute error finds the average absolute magnitude of individual errors between the true value and value predicted by the classifier. The formula below uses the same variables from the MSE formula.\n",
    "\n",
    "$$MAE =\\frac{1}{n} \\sum_{j=1}^{n} \\left\\lvert{P_j - A_j}\\right\\rvert$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank Metrics\n",
    "\n",
    "Rank metrics evaluating classifiers based on how effective they are at separating classes.\n",
    "\n",
    "- Area Under the ROC Curve (AUC) - The area under the ROC curve provides the overall ranking performance of a classifier. The AUC computes the area under the ROC curve and measure the ability of a classifier to distinguish between classes.  The ROC curve shows the true positive rate against the false positive rate for many threshold values.\n",
    "\n",
    "$$AUC =\\frac{S_p - n_p(n_n + 1)/2}{n_p * n_n}$$\n",
    "\n",
    "$$S_p = Sum\\ of\\ all\\ positive\\ instances\\ ranked$$\n",
    "$$n_p = number\\ of\\ positive\\ instances$$\n",
    "$$n_n = number\\ of\\ negative\\ instances$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "\n",
    "https://machinelearningmastery.com/tour-of-evaluation-metrics-for-imbalanced-classification/\n",
    "\n",
    "https://www.researchgate.net/publication/275224157_A_Review_on_Evaluation_Metrics_for_Data_Classification_Evaluations\n",
    "\n",
    "https://www.researchgate.net/publication/275224157_A_Review_on_Evaluation_Metrics_for_Data_Classification_Evaluations\n",
    "\n",
    "https://towardsdatascience.com/20-popular-machine-learning-metrics-part-1-classification-regression-evaluation-metrics-\n",
    "1ca3e282a2ce\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2020/09/precision-recall-machine-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4\n",
    "\n",
    "*Implement a correlation program from scratch to look at the correlations between the features of Admission_Predict.csv dataset file (not provided, you have to download it by yourself by following the instructions in the module Jupyter notebook). Display the correlation matrix where each row and column are the features, which should be an 8 by 8 matrix (should we use 'Serial no'?). You can use pandas DataFrame.corr() to verify correctness of yours. Observe that the diagonal of this matrix should have all 1's and explain why? Since the last column can be used as the target (dependent) variable, what do you think about the correlations between all the variables? Which variable should be the most important for prediction of 'Chance of Admit'?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the diagonal of the correlation matrix is a line of 1's from the top left corner to the bottom right corner.  We should have a diagonal of 1's becauase this shows that each variable correlates with itself.  We can also see that the matrix is symmetrical as the correlation values below the main diagonal mirrors the correlation values above the main diagonal. The last column, Chance of Admit, can be used as the dependent variable. It's interesting that a student having research experience had the lowest correlation with the chance of being admitted (and also appears to have a lower correlation that expected between the other variables). I would have expected that being a research student would have a greater impact on being admitted to a University (as well as having better test scores and a GPA). However, even the Statement of Purpose(SOP) and Letters of Recommendation(LOR) appear to have a greater impact on being admitted to having research experience.  It would be interesting to take a closer look to compare what schools or programs do value having research experience.  As expected, there is a high correlation between both GRE and TOEFL scores.  It makes sense that if a student performed well or badly on one standardized test it would probably mean that they would score similarly on the other standardized exam (hours studied, testing ability, etc).  The cumulative GPA, CGPA, appears to be the most important for the prediction of 'Chance of Admit', as it has the highest correlation with 'Change of Admit' compared to the other variables.  The TOEFL and GRE score are also important to the 'Chance of Admit' after the CGPA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N rows=400, M columns=9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Serial No.</th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Serial No.  GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  \\\n",
       "0           1        337          118                  4  4.5   4.5  9.65   \n",
       "1           2        324          107                  4  4.0   4.5  8.87   \n",
       "2           3        316          104                  3  3.0   3.5  8.00   \n",
       "3           4        322          110                  3  3.5   2.5  8.67   \n",
       "4           5        314          103                  2  2.0   3.0  8.21   \n",
       "\n",
       "   Research  Chance of Admit   \n",
       "0         1              0.92  \n",
       "1         1              0.76  \n",
       "2         1              0.72  \n",
       "3         1              0.80  \n",
       "4         0              0.65  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#checking current directory\n",
    "#print(os.getcwd() + \"\\n\")\n",
    "\n",
    "# change the current directory \n",
    "# to specified directory \n",
    "#os.chdir(r\"C:\\Users\\Sheetal\\Documents\\Sheetal\") \n",
    "\n",
    "#Read in file\n",
    "relative_path = 'datasets/Admission_Predict.csv'\n",
    "df = pd.read_csv(relative_path)\n",
    "\n",
    "# Sanity check\n",
    "print(f'N rows={len(df)}, M columns={len(df.columns)}')\n",
    "\n",
    "#print first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation Using Pandas\n",
    "\n",
    "We can see below that Serial No. does not appear to have any correlation to any of the features as all the correlation coefficients are very low.  We can therefore, remove the Serial No. for the correlation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Serial No.</th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Serial No.</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.097526</td>\n",
       "      <td>-0.147932</td>\n",
       "      <td>-0.169948</td>\n",
       "      <td>-0.166932</td>\n",
       "      <td>-0.088221</td>\n",
       "      <td>-0.045608</td>\n",
       "      <td>-0.063138</td>\n",
       "      <td>0.042336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GRE Score</th>\n",
       "      <td>-0.097526</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.835977</td>\n",
       "      <td>0.668976</td>\n",
       "      <td>0.612831</td>\n",
       "      <td>0.557555</td>\n",
       "      <td>0.833060</td>\n",
       "      <td>0.580391</td>\n",
       "      <td>0.802610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TOEFL Score</th>\n",
       "      <td>-0.147932</td>\n",
       "      <td>0.835977</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.695590</td>\n",
       "      <td>0.657981</td>\n",
       "      <td>0.567721</td>\n",
       "      <td>0.828417</td>\n",
       "      <td>0.489858</td>\n",
       "      <td>0.791594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>University Rating</th>\n",
       "      <td>-0.169948</td>\n",
       "      <td>0.668976</td>\n",
       "      <td>0.695590</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.734523</td>\n",
       "      <td>0.660123</td>\n",
       "      <td>0.746479</td>\n",
       "      <td>0.447783</td>\n",
       "      <td>0.711250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SOP</th>\n",
       "      <td>-0.166932</td>\n",
       "      <td>0.612831</td>\n",
       "      <td>0.657981</td>\n",
       "      <td>0.734523</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.729593</td>\n",
       "      <td>0.718144</td>\n",
       "      <td>0.444029</td>\n",
       "      <td>0.675732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOR</th>\n",
       "      <td>-0.088221</td>\n",
       "      <td>0.557555</td>\n",
       "      <td>0.567721</td>\n",
       "      <td>0.660123</td>\n",
       "      <td>0.729593</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.670211</td>\n",
       "      <td>0.396859</td>\n",
       "      <td>0.669889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CGPA</th>\n",
       "      <td>-0.045608</td>\n",
       "      <td>0.833060</td>\n",
       "      <td>0.828417</td>\n",
       "      <td>0.746479</td>\n",
       "      <td>0.718144</td>\n",
       "      <td>0.670211</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.521654</td>\n",
       "      <td>0.873289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Research</th>\n",
       "      <td>-0.063138</td>\n",
       "      <td>0.580391</td>\n",
       "      <td>0.489858</td>\n",
       "      <td>0.447783</td>\n",
       "      <td>0.444029</td>\n",
       "      <td>0.396859</td>\n",
       "      <td>0.521654</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.553202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chance of Admit</th>\n",
       "      <td>0.042336</td>\n",
       "      <td>0.802610</td>\n",
       "      <td>0.791594</td>\n",
       "      <td>0.711250</td>\n",
       "      <td>0.675732</td>\n",
       "      <td>0.669889</td>\n",
       "      <td>0.873289</td>\n",
       "      <td>0.553202</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Serial No.  GRE Score  TOEFL Score  University Rating  \\\n",
       "Serial No.           1.000000  -0.097526    -0.147932          -0.169948   \n",
       "GRE Score           -0.097526   1.000000     0.835977           0.668976   \n",
       "TOEFL Score         -0.147932   0.835977     1.000000           0.695590   \n",
       "University Rating   -0.169948   0.668976     0.695590           1.000000   \n",
       "SOP                 -0.166932   0.612831     0.657981           0.734523   \n",
       "LOR                 -0.088221   0.557555     0.567721           0.660123   \n",
       "CGPA                -0.045608   0.833060     0.828417           0.746479   \n",
       "Research            -0.063138   0.580391     0.489858           0.447783   \n",
       "Chance of Admit      0.042336   0.802610     0.791594           0.711250   \n",
       "\n",
       "                        SOP      LOR       CGPA  Research  Chance of Admit   \n",
       "Serial No.        -0.166932 -0.088221 -0.045608 -0.063138          0.042336  \n",
       "GRE Score          0.612831  0.557555  0.833060  0.580391          0.802610  \n",
       "TOEFL Score        0.657981  0.567721  0.828417  0.489858          0.791594  \n",
       "University Rating  0.734523  0.660123  0.746479  0.447783          0.711250  \n",
       "SOP                1.000000  0.729593  0.718144  0.444029          0.675732  \n",
       "LOR                0.729593  1.000000  0.670211  0.396859          0.669889  \n",
       "CGPA               0.718144  0.670211  1.000000  0.521654          0.873289  \n",
       "Research           0.444029  0.396859  0.521654  1.000000          0.553202  \n",
       "Chance of Admit    0.675732  0.669889  0.873289  0.553202          1.000000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using Pandas for comparison\n",
    "df.corr(method ='pearson') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GRE Score</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.835977</td>\n",
       "      <td>0.668976</td>\n",
       "      <td>0.612831</td>\n",
       "      <td>0.557555</td>\n",
       "      <td>0.833060</td>\n",
       "      <td>0.580391</td>\n",
       "      <td>0.802610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TOEFL Score</th>\n",
       "      <td>0.835977</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.695590</td>\n",
       "      <td>0.657981</td>\n",
       "      <td>0.567721</td>\n",
       "      <td>0.828417</td>\n",
       "      <td>0.489858</td>\n",
       "      <td>0.791594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>University Rating</th>\n",
       "      <td>0.668976</td>\n",
       "      <td>0.695590</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.734523</td>\n",
       "      <td>0.660123</td>\n",
       "      <td>0.746479</td>\n",
       "      <td>0.447783</td>\n",
       "      <td>0.711250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SOP</th>\n",
       "      <td>0.612831</td>\n",
       "      <td>0.657981</td>\n",
       "      <td>0.734523</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.729593</td>\n",
       "      <td>0.718144</td>\n",
       "      <td>0.444029</td>\n",
       "      <td>0.675732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOR</th>\n",
       "      <td>0.557555</td>\n",
       "      <td>0.567721</td>\n",
       "      <td>0.660123</td>\n",
       "      <td>0.729593</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.670211</td>\n",
       "      <td>0.396859</td>\n",
       "      <td>0.669889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CGPA</th>\n",
       "      <td>0.833060</td>\n",
       "      <td>0.828417</td>\n",
       "      <td>0.746479</td>\n",
       "      <td>0.718144</td>\n",
       "      <td>0.670211</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.521654</td>\n",
       "      <td>0.873289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Research</th>\n",
       "      <td>0.580391</td>\n",
       "      <td>0.489858</td>\n",
       "      <td>0.447783</td>\n",
       "      <td>0.444029</td>\n",
       "      <td>0.396859</td>\n",
       "      <td>0.521654</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.553202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chance of Admit</th>\n",
       "      <td>0.802610</td>\n",
       "      <td>0.791594</td>\n",
       "      <td>0.711250</td>\n",
       "      <td>0.675732</td>\n",
       "      <td>0.669889</td>\n",
       "      <td>0.873289</td>\n",
       "      <td>0.553202</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   GRE Score  TOEFL Score  University Rating       SOP  \\\n",
       "GRE Score           1.000000     0.835977           0.668976  0.612831   \n",
       "TOEFL Score         0.835977     1.000000           0.695590  0.657981   \n",
       "University Rating   0.668976     0.695590           1.000000  0.734523   \n",
       "SOP                 0.612831     0.657981           0.734523  1.000000   \n",
       "LOR                 0.557555     0.567721           0.660123  0.729593   \n",
       "CGPA                0.833060     0.828417           0.746479  0.718144   \n",
       "Research            0.580391     0.489858           0.447783  0.444029   \n",
       "Chance of Admit     0.802610     0.791594           0.711250  0.675732   \n",
       "\n",
       "                       LOR       CGPA  Research  Chance of Admit   \n",
       "GRE Score          0.557555  0.833060  0.580391          0.802610  \n",
       "TOEFL Score        0.567721  0.828417  0.489858          0.791594  \n",
       "University Rating  0.660123  0.746479  0.447783          0.711250  \n",
       "SOP                0.729593  0.718144  0.444029          0.675732  \n",
       "LOR                1.000000  0.670211  0.396859          0.669889  \n",
       "CGPA               0.670211  1.000000  0.521654          0.873289  \n",
       "Research           0.396859  0.521654  1.000000          0.553202  \n",
       "Chance of Admit    0.669889  0.873289  0.553202          1.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removing Serial No.\n",
    "serialNo = df[\"Serial No.\"].values\n",
    "df.drop([\"Serial No.\"],axis=1,inplace = True)\n",
    "\n",
    "#reprint correlation matrix using pandas\n",
    "df.corr(method ='pearson') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a list of the column labels - to be used in correlation matrix formula\n",
    "list = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[337.   118.     4.   ...   9.65   1.     0.92]\n",
      " [324.   107.     4.   ...   8.87   1.     0.76]\n",
      " [316.   104.     3.   ...   8.     1.     0.72]\n",
      " ...\n",
      " [330.   116.     4.   ...   9.45   1.     0.91]\n",
      " [312.   103.     3.   ...   8.78   0.     0.67]\n",
      " [333.   117.     4.   ...   9.66   1.     0.95]]\n"
     ]
    }
   ],
   "source": [
    "#converting pandas dataframe to numpy array\n",
    "df_numpy = df.to_numpy()\n",
    "print(df_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other Method to find Correlation Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the correlation matrix, first the covariance formula is defined to then be used in teh correlation matrix function.  As can be seen below, my funtion for calculating the correlation matrix, produces similar results to the correlation matrix created when using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#covariance formula\n",
    "def covariance(x, y): \n",
    "  \n",
    "    #setting the mean of x and y\n",
    "    E_x = x.mean() \n",
    "    E_y = y.mean() \n",
    "    \n",
    "    #finding the number of data values\n",
    "    n = len(x) \n",
    "    \n",
    "    #returning the formula for covariance\n",
    "    return sum((x - E_x) * (y - E_y)) / n "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.8359768 , 0.66897585, 0.61283074, 0.55755452,\n",
       "        0.83306045, 0.58039064, 0.80261046],\n",
       "       [0.8359768 , 1.        , 0.69558984, 0.65798053, 0.56772092,\n",
       "        0.82841742, 0.48985785, 0.79159399],\n",
       "       [0.66897585, 0.69558984, 1.        , 0.73452277, 0.66012345,\n",
       "        0.74647869, 0.44778251, 0.71125025],\n",
       "       [0.61283074, 0.65798053, 0.73452277, 1.        , 0.72959254,\n",
       "        0.71814396, 0.44402881, 0.67573186],\n",
       "       [0.55755452, 0.56772092, 0.66012345, 0.72959254, 1.        ,\n",
       "        0.6702113 , 0.39685926, 0.66988879],\n",
       "       [0.83306045, 0.82841742, 0.74647869, 0.71814396, 0.6702113 ,\n",
       "        1.        , 0.52165423, 0.8732891 ],\n",
       "       [0.58039064, 0.48985785, 0.44778251, 0.44402881, 0.39685926,\n",
       "        0.52165423, 1.        , 0.55320214],\n",
       "       [0.80261046, 0.79159399, 0.71125025, 0.67573186, 0.66988879,\n",
       "        0.8732891 , 0.55320214, 1.        ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Correlation formula: Covariance Formula / (standard deviation of x) * (standard deviation of y)\n",
    "\n",
    "def correlation(input): \n",
    "  \n",
    "    #getting number of rows and columns\n",
    "    rows, columns = input.shape \n",
    "    \n",
    "    #creating empty matrix with dimension of columns x columns\n",
    "    correlation_matrix = np.zeros((columns, columns)) \n",
    "  \n",
    "    for i in range(columns): \n",
    "  \n",
    "        for j in range(columns): \n",
    "  \n",
    "            x = input[:, i]\n",
    "            y = input[:, j] \n",
    "    #entering correlation values into empty matrix \n",
    "            correlation_matrix[i][j] = covariance(x, y) / (x.std() * y.std()) \n",
    "  \n",
    "    return correlation_matrix\n",
    "\n",
    "#printing resulting numpy array\n",
    "correlation(df_numpy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting numpy correlation matrix to variable\n",
    "final_matrix = correlation(df_numpy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  GRE Score TOEFL Score University Rating       SOP      LOR   \\\n",
      "GRE Score          1.000000    0.835977          0.668976  0.612831  0.557555   \n",
      "TOEFL Score        0.835977    1.000000          0.695590  0.657981  0.567721   \n",
      "University Rating  0.668976    0.695590          1.000000  0.734523  0.660123   \n",
      "SOP                0.612831    0.657981          0.734523  1.000000  0.729593   \n",
      "LOR                0.557555    0.567721          0.660123  0.729593  1.000000   \n",
      "CGPA               0.833060    0.828417          0.746479  0.718144  0.670211   \n",
      "Research           0.580391    0.489858          0.447783  0.444029  0.396859   \n",
      "Chance of Admit    0.802610    0.791594          0.711250  0.675732  0.669889   \n",
      "\n",
      "                       CGPA  Research Chance of Admit   \n",
      "GRE Score          0.833060  0.580391         0.802610  \n",
      "TOEFL Score        0.828417  0.489858         0.791594  \n",
      "University Rating  0.746479  0.447783         0.711250  \n",
      "SOP                0.718144  0.444029         0.675732  \n",
      "LOR                0.670211  0.396859         0.669889  \n",
      "CGPA               1.000000  0.521654         0.873289  \n",
      "Research           0.521654  1.000000         0.553202  \n",
      "Chance of Admit    0.873289  0.553202         1.000000  \n"
     ]
    }
   ],
   "source": [
    "#converting numpy correlation matrix to pandas dataframe\n",
    "#list generated from column labels or original dataframe\n",
    "result = pd.DataFrame(final_matrix,\n",
    "                     index= [list], \n",
    "                     columns=[list])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "\n",
    "https://java2blog.com/convert-numpy-array-to-pandas-dataframe/\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_numpy.html\n",
    "\n",
    "https://cmdlinetips.com/2020/04/how-to-get-column-names-as-list-in-pandas/\n",
    "\n",
    "https://www.w3schools.com/python/numpy_array_shape.asp\n",
    "\n",
    "https://www.geeksforgeeks.org/numpy-zeros-python/\n",
    "\n",
    "https://stackoverflow.com/questions/18688948/numpy-how-do-i-find-total-rows-in-a-2d-array-and-total-column-in-a-1d-array"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
