{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied Machine Learning: Assignment \\#09"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sheetal Parikh\n",
    "EN.605.631.81<br>\n",
    "April 5, 2021\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Please refer to the Reinforcement Learning Jupyter notebook in course materials.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "*Describe the environment in the Nim learning model.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment consists of the states, actions, and rewards of the system.  In the Nim learning model, the environment of the game can be visualized as a 3 x 11 board.  The 3 represents the three possible piles to choose from and the 11 represents $s_{t}$ + 1.  The variable, $s_{t}$ represents the the max number items per pile (which in our example is 10).  We are adding a 1 for if a pile has 0 items. The states are all of the possible choices from the board.  For example, one state would be a player removing 2 items from pile 3 (state = (3,2)).  Because the model has a finite number of states, the model also has a finite number of possible actions. The states and rewards are inputs of the Q-learning agent and the actions are the output. A further description of the rewards, states, and actions are stated in the other problems below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Problem 2\n",
    "\n",
    "*Describe the agent in the Nim learning model.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the agent in the Nim learning model is to determine the optimal method of removing items from the pile so that the last pile is cleared.  In this model, the agent can only remove items from the pile and can take anything from 1 item up to the amount of items left in the pile.  Also, the agent can only take items from the same pile in one turn.  The agent learns about the environment by updating the Q values and wants to identify the highest quality action in any given state. The Q-values estimate how much additional reward we can expect to accumulate through all the remaining steps in the episode if the agent is in a state and takes a certain action.  As the agent learns, it will always choose the action with the largest Q-value.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Problem 3\n",
    "\n",
    "*Describe the reward and penalty in the Nim learning model.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the current Nim learning model, a reward of 100 is only given for the move that wins the game.  There isn't a penalty for losing the game.  Equation 3 from the module 9 notebook helps find the optimal policy for maximizing the expected value of the total reward.  The discount factor, gamma, prevents the total reward from going to infinity and since it is set at 0.80, it helps give higher weight to moves that give future rewards.  The alpha, or learning rate, determines how much of the new learned value would be used and is set at 1 which gives higher weight to recent rewards. In the model, the alpha is set to one since every winning state must be learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Problem 4\n",
    "\n",
    "*How many possible states there could be in the Nim game with a maximum of 10 items per pile and 3 piles total?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine the number of possible states, we would find the product of the number of states per pile.  As stated in problem 1, each pile has a 11 states becauase we have 10 as the max items per pile plus 1 for if the the pile has 0 items remaining.  Therefore, in our Nim game, the possible states would be the following if we have 3 states:\n",
    "\n",
    "(10 + 1) + (10 + 1) + (10 + 1) = 11 * 11 * 11 = $11^{3}$ = 1331 total possible states.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Problem 5\n",
    "\n",
    "*How many possible actions there could be in the Nim game with 10 items per pile and 3 piles total?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Nim game, the agent can only remove items from the piles.  Therefore, to determine the possible actions in the Nim game with 10 items per pile and 3 piles, we would find the sum of the number of possible items per every pile.  For our example, the total possible actions would be:\n",
    "\n",
    "10 + 10 + 10 = 30 total possible actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Problem 6\n",
    "\n",
    "*Find a way to improve the provided Nim game learning model. Do you think one can beat the Guru player? (Hint: How about penalizing the losses? Hint: It is indeed possible to find a better solution, which improves the way Q-learning updates its Q-table).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using code from Module 9 notebook\n",
    "\n",
    "import numpy as np\n",
    "from random import randint, choice\n",
    "\n",
    "# The number of piles is 3\n",
    "\n",
    "#number of games/episodes\n",
    "n_episodes = 10000\n",
    "\n",
    "# max number of items per pile\n",
    "ITEMS_MX = 10\n",
    "\n",
    "# Initialize starting position\n",
    "def init_game():\n",
    "    return [randint(1,ITEMS_MX), randint(1,ITEMS_MX), randint(1,ITEMS_MX)]\n",
    "\n",
    "# Based on X-oring the item counts in piles - mathematical solution\n",
    "def nim_guru(st):\n",
    "    xored = st[0] ^ st[1] ^ st[2]\n",
    "    if xored == 0:\n",
    "        return nim_random(st)\n",
    "    #\n",
    "    for pile in range(3):\n",
    "        s = st[pile] ^ xored\n",
    "        if s <= st[pile]:\n",
    "            return st[pile]-s, pile\n",
    "\n",
    "# Random Nim player\n",
    "def nim_random(_st):\n",
    "    pile = choice([i for i in range(3) if _st[i]>0])  # find the non-empty piles\n",
    "    return randint(1, _st[pile]), pile  # random move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using code from Module 9 notebook\n",
    "\n",
    "def nim_qlearner(_st):\n",
    "    # pick the best rewarding move, equation 1\n",
    "    a = np.argmax(qtable[_st[0], _st[1], _st[2]])  # exploitation\n",
    "    # index is based on move, pile\n",
    "    move, pile = a%ITEMS_MX+1, a//ITEMS_MX\n",
    "    # check if qtable has generated a random but game illegal move - we have not explored there yet\n",
    "    if move <= 0 or _st[pile] < move:\n",
    "        move, pile = nim_random(_st)  # exploration\n",
    "    #\n",
    "    return move, pile  # action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using code from Module 9 notebook\n",
    "\n",
    "Engines = {'Random':nim_random, 'Guru':nim_guru, 'Qlearner':nim_qlearner}\n",
    "\n",
    "\n",
    "def game(a, b):\n",
    "    state, side = init_game(), 'A'\n",
    "    while True:\n",
    "        engine = Engines[a] if side == 'A' else Engines[b]\n",
    "        move, pile = engine(state)\n",
    "        # print(state, move, pile)  # debug purposes\n",
    "        state[pile] -= move\n",
    "        if state == [0, 0, 0]:  # game ends\n",
    "            return side  # winning side\n",
    "        #\n",
    "        side = 'B' if side == 'A' else 'A'  # switch sides\n",
    "\n",
    "def play_games(_n, a, b):\n",
    "    from collections import defaultdict\n",
    "    wins = defaultdict(int)\n",
    "    for i in range(_n):\n",
    "        wins[game(a, b)] += 1\n",
    "    # info\n",
    "    print(f\"{_n} games, {a:>8s} {wins['A']:5d}  vs {b:>8s}{wins['B']:5d}      : \"\n",
    "                      f\"{a:>8s} Win % = {round(100*(wins['A']/(wins['A'] + wins['B'])),1)}\")\n",
    "    #\n",
    "    return wins['A'], wins['B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using code from Module 9 notebook\n",
    "\n",
    "#qtable_loss and qtable_loss2 for the two new models\n",
    "qtable, qtable_loss, qtable_loss2, Alpha, Gamma, Reward = None, None, None, 1.0, 0.80, 100.0\n",
    "\n",
    "# learn from _n games, randomly played to explore the possible states\n",
    "def nim_qlearn(_n):\n",
    "    global qtable\n",
    "    # based on max items per pile\n",
    "    qtable = np.zeros((ITEMS_MX+1, ITEMS_MX+1, ITEMS_MX+1, ITEMS_MX*3), dtype=float)\n",
    "    # play _n games\n",
    "    for i in range(_n):\n",
    "        # first state is starting position\n",
    "        st1 = init_game()\n",
    "        while True:  # while game not finished\n",
    "            # make a random move - exploration\n",
    "            move, pile = nim_random(st1)\n",
    "            st2 = list(st1)\n",
    "            # make the move\n",
    "            st2[pile] -= move  # --> last move I made\n",
    "            if st2 == [0, 0, 0]:  # game ends\n",
    "                qtable_update(Reward, st1, move, pile, 0)  # I won\n",
    "                break  # new game\n",
    "            #\n",
    "            qtable_update(0, st1, move, pile, np.max(qtable[st2[0], st2[1], st2[2]]))\n",
    "            st1 = st2\n",
    "            \n",
    "# Equation 3 - update the qtable\n",
    "def qtable_update(r, _st1, move, pile, q_future_best):\n",
    "    a = pile*ITEMS_MX+move-1\n",
    "    qtable[_st1[0], _st1[1], _st1[2], a] = Alpha * (r + Gamma * q_future_best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training model\n",
    "nim_qlearn(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using code from Module 9 notebook\n",
    "\n",
    "#improved Q-table update incorporating penalty amd using random exploration\n",
    "\n",
    "def nim_qlearn_wloss(_n):\n",
    "    global qtable_loss\n",
    "    # based on max items per pile\n",
    "    qtable_loss = np.zeros((ITEMS_MX+1, ITEMS_MX+1, ITEMS_MX+1, ITEMS_MX*3), dtype=float)\n",
    "    # play _n games\n",
    "    for i in range(_n):\n",
    "        # first state is starting position\n",
    "        st1 = init_game()\n",
    "        #for the losing move\n",
    "        st0 = None\n",
    "        earlier_move = None\n",
    "        earlier_pile = None\n",
    "        while True:  # while game not finished\n",
    "            # make a random move - exploration\n",
    "            #move, pile = nim_guru(st1)\n",
    "            #move, pile = explore(st1)\n",
    "            move, pile = nim_random(st1)\n",
    "            st2 = list(st1)\n",
    "            # make the move\n",
    "            st2[pile] -= move  # --> last move I made\n",
    "            if st2 == [0, 0, 0]:  # game ends\n",
    "                qtable_update2(Reward, st1, move, pile, 0)  # winning\n",
    "                #qtable_update2(-100, st0, earlier_move, earlier_pile, 0)  # losing\n",
    "                #reward based on difference in next state value and the current state’s value\n",
    "                qtable_update2((Alpha * (Reward + Gamma * np.max(qtable_loss[st1]) - np.max(qtable_loss[st0]))), \n",
    "                               st0, earlier_move, earlier_pile, 0)  # losing\n",
    "                break  # new game\n",
    "            #\n",
    "            qtable_update2(0, st1, move, pile, np.max(qtable_loss[st2[0], st2[1], st2[2]]))\n",
    "            earlier_move = move\n",
    "            earlier_pile = pile\n",
    "            st0 = st1\n",
    "            st1 = st2\n",
    "\n",
    "# Equation 3 - update the qtable\n",
    "def qtable_update2(r, _st1, move, pile, q_future_best):\n",
    "    a = pile*ITEMS_MX+move-1\n",
    "    qtable_loss[_st1[0], _st1[1], _st1[2], a] = Alpha * (r + Gamma * q_future_best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using code from Module 9 notebook\n",
    "\n",
    "def nim_qlearner_loss(_st):\n",
    "    # pick the best rewarding move, equation 1\n",
    "    a = np.argmax(qtable_loss[_st[0], _st[1], _st[2]])  # exploitation\n",
    "    # index is based on move, pile\n",
    "    move, pile = a%ITEMS_MX+1, a//ITEMS_MX\n",
    "    # check if qtable has generated a random but game illegal move - we have not explored there yet\n",
    "    if move <= 0 or _st[pile] < move:\n",
    "        move, pile = nim_random(_st)  # exploration\n",
    "    #\n",
    "    return move, pile  # action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using code from Module 9 notebook\n",
    "\n",
    "#improved Q-table update incorporating penalty and using nim-guru exploration\n",
    "\n",
    "def nim_qlearn_wloss2(_n):\n",
    "    global qtable_loss2\n",
    "    # based on max items per pile\n",
    "    qtable_loss2 = np.zeros((ITEMS_MX+1, ITEMS_MX+1, ITEMS_MX+1, ITEMS_MX*3), dtype=float)\n",
    "    # play _n games\n",
    "    for i in range(_n):\n",
    "        # first state is starting position\n",
    "        st1 = init_game()\n",
    "        #for the losing move\n",
    "        st0 = None\n",
    "        earlier_move = None\n",
    "        earlier_pile = None\n",
    "        while True:  # while game not finished\n",
    "            # make a move - nim guru exploration\n",
    "            move, pile = nim_guru(st1)\n",
    "            #move, pile = nim_random(st1)\n",
    "            st2 = list(st1)\n",
    "            # make the move\n",
    "            st2[pile] -= move  # --> last move I made\n",
    "            if st2 == [0, 0, 0]:  # game ends\n",
    "                qtable_update3(Reward, st1, move, pile, 0)  # woinning\n",
    "                #qtable_update3(-100, st0, earlier_move, earlier_pile, 0)  # losing\n",
    "                #reward based on difference in next state value and the current state’s value\n",
    "                qtable_update3((Alpha * (Reward + Gamma * np.max(qtable_loss2[st1]) - np.max(qtable_loss2[st0]))), \n",
    "                               st0, earlier_move, earlier_pile, 0)  # losing\n",
    "                break  # new game\n",
    "            #\n",
    "            qtable_update3(0, st1, move, pile, np.max(qtable_loss2[st2[0], st2[1], st2[2]]))\n",
    "            earlier_move = move\n",
    "            earlier_pile = pile\n",
    "            st0 = st1\n",
    "            st1 = st2\n",
    "\n",
    "# Equation 3 - update the qtable\n",
    "def qtable_update3(r, _st1, move, pile, q_future_best):\n",
    "    a = pile*ITEMS_MX+move-1\n",
    "    qtable_loss2[_st1[0], _st1[1], _st1[2], a] = Alpha * (r + Gamma * q_future_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using code from Module 9 notebook\n",
    "\n",
    "def nim_qlearner_loss2(_st):\n",
    "    # pick the best rewarding move, equation 1\n",
    "    a = np.argmax(qtable_loss2[_st[0], _st[1], _st[2]])  # exploitation\n",
    "    # index is based on move, pile\n",
    "    move, pile = a%ITEMS_MX+1, a//ITEMS_MX\n",
    "    # check if qtable has generated a random but game illegal move - we have not explored there yet\n",
    "    if move <= 0 or _st[pile] < move:\n",
    "        move, pile = nim_random(_st)  # exploration\n",
    "    #\n",
    "    return move, pile  # action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 57 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#adding new models to Engines and training \n",
    "\n",
    "#adding to Engines\n",
    "Engines['Qlearner_wloss'] = nim_qlearner_loss\n",
    "Engines['Qlearner_wloss2'] = nim_qlearner_loss2\n",
    "\n",
    "#training\n",
    "nim_qlearn_wloss(100000)\n",
    "nim_qlearn_wloss2(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 games,     Guru  9371  vs     Guru  629      :     Guru Win % = 93.7\n",
      "10000 games,     Guru  9980  vs   Random   20      :     Guru Win % = 99.8\n",
      "10000 games,     Guru  9983  vs Qlearner   17      :     Guru Win % = 99.8\n",
      "10000 games,     Guru  9979  vs Qlearner_wloss   21      :     Guru Win % = 99.8\n",
      "10000 games,     Guru  9389  vs Qlearner_wloss2  611      :     Guru Win % = 93.9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9389, 611)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Play games when Guru goes first\n",
    "#n_episodes = 10000\n",
    "\n",
    "play_games(n_episodes, 'Guru', 'Guru')\n",
    "play_games(n_episodes, 'Guru', 'Random') ;\n",
    "play_games(n_episodes, 'Guru', 'Qlearner') ;\n",
    "play_games(n_episodes, 'Guru', 'Qlearner_wloss')\n",
    "play_games(n_episodes, 'Guru', 'Qlearner_wloss2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 games,   Random  5001  vs   Random 4999      :   Random Win % = 50.0\n",
      "10000 games,   Random    78  vs     Guru 9922      :   Random Win % = 0.8\n",
      "10000 games,   Random  2972  vs Qlearner 7028      :   Random Win % = 29.7\n",
      "10000 games,   Random  2955  vs Qlearner_wloss 7045      :   Random Win % = 29.5\n",
      "10000 games,   Random    99  vs Qlearner_wloss2 9901      :   Random Win % = 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(99, 9901)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#play games when Random goes first\n",
    "\n",
    "play_games(n_episodes, 'Random', 'Random')\n",
    "play_games(n_episodes, 'Random', 'Guru') ;\n",
    "play_games(n_episodes, 'Random', 'Qlearner') ;\n",
    "play_games(n_episodes, 'Random', 'Qlearner_wloss')\n",
    "play_games(n_episodes, 'Random', 'Qlearner_wloss2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 games, Qlearner 10000  vs Qlearner    0      : Qlearner Win % = 100.0\n",
      "10000 games, Qlearner   316  vs     Guru 9684      : Qlearner Win % = 3.2\n",
      "10000 games, Qlearner  7211  vs   Random 2789      : Qlearner Win % = 72.1\n",
      "10000 games, Qlearner 10000  vs Qlearner_wloss    0      : Qlearner Win % = 100.0\n",
      "10000 games, Qlearner  1017  vs Qlearner_wloss2 8983      : Qlearner Win % = 10.2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1017, 8983)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#play games when Qlearner goes first\n",
    "\n",
    "play_games(n_episodes, 'Qlearner', 'Qlearner')\n",
    "play_games(n_episodes, 'Qlearner', 'Guru') ;\n",
    "play_games(n_episodes, 'Qlearner', 'Random') ;\n",
    "play_games(n_episodes, 'Qlearner', 'Qlearner_wloss')\n",
    "play_games(n_episodes, 'Qlearner', 'Qlearner_wloss2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 games, Qlearner_wloss 10000  vs Qlearner_wloss    0      : Qlearner_wloss Win % = 100.0\n",
      "10000 games, Qlearner_wloss  7234  vs   Random 2766      : Qlearner_wloss Win % = 72.3\n",
      "10000 games, Qlearner_wloss 10000  vs Qlearner    0      : Qlearner_wloss Win % = 100.0\n",
      "10000 games, Qlearner_wloss   312  vs     Guru 9688      : Qlearner_wloss Win % = 3.1\n",
      "10000 games, Qlearner_wloss   987  vs Qlearner_wloss2 9013      : Qlearner_wloss Win % = 9.9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(987, 9013)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#play games when improved Qlearner_wLoss goes first\n",
    "\n",
    "play_games(n_episodes, 'Qlearner_wloss', 'Qlearner_wloss')\n",
    "play_games(n_episodes, 'Qlearner_wloss', 'Random') ;\n",
    "play_games(n_episodes, 'Qlearner_wloss', 'Qlearner') ;\n",
    "play_games(n_episodes, 'Qlearner_wloss', 'Guru')\n",
    "play_games(n_episodes, 'Qlearner_wloss', 'Qlearner_wloss2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 games, Qlearner_wloss2  9395  vs Qlearner_wloss2  605      : Qlearner_wloss2 Win % = 94.0\n",
      "10000 games, Qlearner_wloss2  9972  vs   Random   28      : Qlearner_wloss2 Win % = 99.7\n",
      "10000 games, Qlearner_wloss2 10000  vs Qlearner    0      : Qlearner_wloss2 Win % = 100.0\n",
      "10000 games, Qlearner_wloss2  9408  vs     Guru  592      : Qlearner_wloss2 Win % = 94.1\n",
      "10000 games, Qlearner_wloss2 10000  vs Qlearner_wloss    0      : Qlearner_wloss2 Win % = 100.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10000, 0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play_games(n_episodes, 'Qlearner_wloss2', 'Qlearner_wloss2')\n",
    "play_games(n_episodes, 'Qlearner_wloss2', 'Random') ;\n",
    "play_games(n_episodes, 'Qlearner_wloss2', 'Qlearner') ;\n",
    "play_games(n_episodes, 'Qlearner_wloss2', 'Guru')\n",
    "play_games(n_episodes, 'Qlearner_wloss2', 'Qlearner_wloss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried two Nim learning models. For both models, the winning move continues to receive a reward of 100.  However the losing move receives a reward that is applied to the difference between the discounted value of the move in the next state and the current state’s value.  Therefore, \"better\" bad moves will have higher rewards. The gamma discount rate is also applied to make sure no moves are heaviliy weighed.  When I tried adding a penalty of a set value, for example, -100, the model performance worsened.  The first model, uses the random method of exploration.  This model doesn't seem to have any improvement to the original nim learner model. I wasn't able to figure out how to improve the model further.  Therefore, the second model, used the nim guru engine to explore the possible states. This model had an above 90% winning rate against the Guru player but only because it used the nim-guru engine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/getting-started-with-reinforcement-q-learning-77499b1766b6\n",
    "\n",
    "https://medium.com/@edkins.sarah/intro-to-how-q-table-agents-learn-26c163e42db7\n",
    "\n",
    "https://www.duo.uio.no/bitstream/handle/10852/73236/Solving-Nim-by-the-Use-of-Machine-Learning.pdf?sequence=1&isAllowed=y\n",
    "\n",
    "https://towardsdatascience.com/q-learning-54b841f3f9e4\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
